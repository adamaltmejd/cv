@article {Munafo2015,
	author = {Munafo, Marcus R. and Pfeiffer, Thomas and Altmejd, Adam and Heikensten, Emma and Almenberg, Johan and Bird, Alexander and Chen, Yiling and Wilson, Brad and Johannesson, Magnus and Dreber, Anna},
	title = {Using prediction markets to forecast research evaluations},
	volume = {2},
	number = {10},
	year = {2015},
	doi = {10.1098/rsos.150287},
	publisher = {The Royal Society},
	abstract = {The 2014 Research Excellence Framework (REF2014) was conducted to assess the quality of research carried out at higher education institutions in the UK over a 6 year period. However, the process was criticized for being expensive and bureaucratic, and it was argued that similar information could be obtained more simply from various existing metrics. We were interested in whether a prediction market on the outcome of REF2014 for 33 chemistry departments in the UK would provide information similar to that obtained during the REF2014 process. Prediction markets have become increasingly popular as a means of capturing what is colloquially known as the {\textquoteleft}wisdom of crowds{\textquoteright}, and enable individuals to trade {\textquoteleft}bets{\textquoteright} on whether a specific outcome will occur or not. These have been shown to be successful at predicting various outcomes in a number of domains (e.g. sport, entertainment and politics), but have rarely been tested against outcomes based on expert judgements such as those that formed the basis of REF2014.},
	URL = {http://rsos.royalsocietypublishing.org/content/2/10/150287},
	eprint = {http://rsos.royalsocietypublishing.org/content/2/10/150287.full.pdf},
	journal = {Royal Society Open Science}
}
@article {Camerer2016,
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	title = {Evaluating replicability of laboratory experiments in economics},
	year = {2016},
	doi = {10.1126/science.aaf0918},
	publisher = {American Association for the Advancement of Science},
	abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918},
	eprint = {http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918.full.pdf},
	journal = {Science}
}

